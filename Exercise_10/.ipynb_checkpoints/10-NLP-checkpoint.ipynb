{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIS 9\n",
    "## Natural Language Processing with NLTK\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading\n",
    "<br>Natural Language Processing with Python: Learning to classify text, up to section 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Natural language_ is the language that humans use to communicate with each other. It's the language we use in our conversation or when we read and write texts, as opposed to computer languages.\n",
    "\n",
    "_Natural Language Processing (NLP)_ or _Computational Linguistics_ is a field that uses computers to analyze, understand, and generate natural human language. NLP is used for tasks from counting word frequencies to allowing computers to interpret human commands to translating languages. NLP uses machine learning algorithms to build probabilistic model about a language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Common NLTK tools for pre-processing text\n",
    "<br>Since texts are human readable words while machines only work with numbers, often in NLP we need to transform the text into meaningful numbers and possibly reduce the number of words in the text before we can use machine learning algorithms with the text. This is called pre-processing the text, and it's similar to the step in clasic machine learning where we change a feature description (with words) into a corresponding sequence of numbers, or when we use dimensional reduction to remove the number of irrelevant or duplicate features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Tokenize__: separate the text into words or sentences. A sentence, paragraph, or an entire essay or writing is  made of words that give meaning to the text, so the dataset in NLP is generally the words in the text. A _tokenizer_ makes it simple to separate the text into words.\n",
    "\n",
    "NTLK has many packages and not all available packages are downloaded by default, so we need to download and install the tokenizer the first time we use it. \n",
    "<br>(If you forget to comment out the download line of code after the first time, don't worry, the package manager is smart enough to figure out that you've downloaded it already.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')     # only need to run this one time to download and install the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample line of text from python.org\n",
    "s = '''Python is a programming language that lets you work quickly and integrate systems more effectively.\n",
    "It's a programmer friendly language.'''  # adding a second line of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize   # separate text into words\n",
    "\n",
    "words = word_tokenize(s)\n",
    "print(words)\n",
    "\n",
    "# a. How does this compare with Python string split() method?                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "only_words = tokenizer.tokenize(s)\n",
    "print(only_words)\n",
    "\n",
    "# b. How is this list of words diferent from the previous list?                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Frequency distribution__: count of occurrences of words. Sometimes looking at the words that are most common in the text can give us some general idea about the text: is it casual or formal, what could be the main topics, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqD = nltk.FreqDist(only_words)\n",
    "for key,val in freqD.items():\n",
    "    print(str(key) + ':' + str(val))\n",
    "print(len(freqD))\n",
    "\n",
    "# We know enough Python to easily write the code for FreqDist, but it's nice to just call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqD.plot(20)    \n",
    "\n",
    "# What happens when the input argument is 10?                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __Stop words__: words that appear often in the text but don't necessarily help us find the meaning of the text. Example stop words are 'an', 'the', 'to', 'that'.\n",
    "<br>Sometimes it's useful to remove the stop words because it can reduce the size of the datasets, making it faster to train the algorithm. But sometimes it's a bad idea to remove stop words if they actually have meaning in the text. For classification problems it's generally fine to remove the stop words.\n",
    "<br>The list of stop words are different for different NLP tools, NLTK has different stop words than other tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to download and install the NLTK package for stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')      # only need to run this one time to download and install the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of words from only_words above, but with no stop words           \n",
    "\n",
    "print(\"Words:\",only_words)\n",
    "print()\n",
    "print(\"Without stop words:\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. __Stemming__: removing parts of a word to get to its _stem_ or root. This can be useful to reduce the number of words we need to analyze. For example: 'talk', 'talking', 'talked', 'talks' all have the same stem 'talk' and perhaps can be grouped together. That way they are only counted once, or that way we can count their frequency as a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed=[stemmer.stem(w) for w in only_words]\n",
    "print(\"Words:\",only_words)\n",
    "print()\n",
    "print(\"Stems:\",stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. __Part of speech (POS) tagging__: find the part of speech for each word. Sometime it's important to identify the part of speech of a word because it can make a difference in the meaning of a word. For example, 'drive' can be a verb in 'They drive fast', or 'drive' can be a noun in 'The drive was long and hot'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to download and install the NLTK package for tagging POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')    # only need to run this one time to download and install the package\n",
    "# The name perceptron hints at the fact that the tagger is done with a neural network. \n",
    "# More on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordTags = nltk.pos_tag(only_words)   \n",
    "print(wordTags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of tags and their meanings are [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From wordTags above, find all the verbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first explore NLTK's own machine learning algorithm, the Naive Bayes model, for text classification. Naive Bayes is one of the most common algorithm to use with text because it can be fairly accurate with the prediction without needing a large dataset. It was initially introduced for text classification tasks and is still used as a benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Predicting names__\n",
    "<br>We start with a simple learning algorithm that works with a list of people's first names and genders, then we train the algorithm to predict the gender of a person, given the person's name. In this simple case the names are one-word features and capitalized correctly, so we don't need to do any pre-processing of data. We don't need to:\n",
    "- make the name case insensitive by converting them to lowercase\n",
    "- tokenize the names since they are already single words\n",
    "- the names are proper nouns so no need to remove stop words or reduce them to stems or determine the part of speech\n",
    "\n",
    "In the English language, male first names and female first names tend to have different endings or different last letters. Names ending in a, e and i are likely to be female, while names ending in k, o, r, s and t are likely to be male. We will build a classifier to check the accuracy of predicting whether a name is male or female based on the last letter of the name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Download and install the names library from NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('names')    # (only need to run this one time to download and install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Get a list of male and female names from the NLTK library, assign labels to them, and randomize the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    " \n",
    "allnames = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "         [(name, 'female') for name in names.words('female.txt')])\n",
    "print(len(allnames))\n",
    "print(allnames[:2], allnames[-2:])   # alphabetical order\n",
    "random.shuffle(allnames)          # random order\n",
    "print(allnames[:2], allnames[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. The ntlk Naive Bayes model is trained with a _feature set_, which is a list of tuples, where each tuple is the input feature and corresponding output label.\n",
    "<br>An input feature is a dictionary where the key is a short text description, and the value is a single boolean or integer (character). The labels can be text strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeature(word): \n",
    "    return {'lett': word[-1]}  # use last letter as feature\n",
    "\n",
    "# Using the function getFeature above, which returns a dictionary, create the list \n",
    "# which is the feature set (the feature set is described in the cell above).\n",
    "# Example of one element of the feature set: \n",
    "# If the name is ('Alane', 'female'), then the corresponding element in the \n",
    "# feature set is: ({'lett':'e'},'female')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Split the dataset into a training and testing set, then use the training set to train the classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "train_set, test_set = train_test_split(featureSet,test_size=0.2)\n",
    "print(len(train_set), len(test_set))\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Check the classifier accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bag of Words model__\n",
    "<br>In this next example we work with a larger problem where we train a model to predict whether SMS messages are spam or not. Even though SMS messages are generally short, they are sentences with multiple words. This means the dataset is large and will need to be pre-processed before they are converted to numbers to work with a ML model.\n",
    "\n",
    "A bag of words model is the simplest way to analyze data. It reduces the text to a container of words, without considering the order of the words or how the words are put together in a sentence (such as, is it part of a question or a statement or an exclamation). Only the number of occurrences or frequency of each word is counted.\n",
    "\n",
    "Bag of words model is useful in classification to determine whether the text is a particular type of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Just like \"Hello world\" is the intro to programming, and \"Ice cream sales\" is the intro to machine learning, \"Ham vs. spam\" is the intro to NLP.\n",
    "<br>We will try to classify whether an SMS is spam or ham (not spam). We use the input file _spam_short.csv_ from [source](https://www.kaggle.com/uciml/sms-spam-collection-dataset) that has been shortened to the first 1000 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputData = pd.read_csv('spam_short.csv', encoding='ISO-8859-1')   # Unicode characters\n",
    "inputData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Remove the extra columns with NaN and change the column labels from 'v1' and 'v2' to 'label' and 'text'.\n",
    "<br>Print the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Create the y labels from the 'label' column and convert 'ham' to 0 and 'spam' to 1\n",
    "<br>Show the size of y and the first few lines of y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Create the X Dataframe from the 'text' column.\n",
    "<br>Print the size of X and the first few lines of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Use the pre-processing techniques in part A to reduce the X dataset:\n",
    "- remove all numbers and punctuations (ony words with letters should remain)\n",
    "- change all words to lowercase\n",
    "- remove all stop words\n",
    "- stem all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EC: write code to do the steps in Q15. \n",
    "# You should end up with an X DataFrame, name it X_processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Transform each word into a document vector, which is a way to give a score to each word, by using the _CountVectorizer_\n",
    "<br>The CountVectorizer counts the frequency of each word, and returns a vector of numbers (features) that can be used by a machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how a CountVectorizer works\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"The quick brown fox jumps over the lazy dog\"]  # classic typing exercise\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(text)      \n",
    "print(\"Frequency count:\", vectorizer.vocabulary_)\n",
    "\n",
    "vector = vectorizer.transform(text)   # output vector to be used by ML algorithms\n",
    "print(vector.shape)\n",
    "print(vector)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Now we apply CountVectorizer to our X_processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit(X_processed[0])\n",
    "X_vectors = vect.transform(X_processed[0])\n",
    "print(X_vectors[0])\n",
    "X_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Split the X_vectors and y datasets into training and testing sets and print the size of the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. Recall that Multinomial Naive Bayes take each feature and its frequency into account to do output prediction, so it fits very well with the X_vectors. We train the classifier and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. Check the accuracy of the model and show the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "print(metrics.accuracy_score(y_test, y_pred))\n",
    "metrics.confusion_matrix(y_test, y_pred, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is a big and challenging field because natural languages are messy, there are as many exceptions as there are rules in human languages, and languages evolve quickly. But it's also a field that is growing in importance with the pervasive role of social media and the advances of smart devices that interact us by using our human languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Optional_ note: We saw that the POS tagger is created from a neural network or perceptron. For those who are interested in reading an example of creating a neural network to do POS tagging, see this [article](https://becominghuman.ai/part-of-speech-tagging-tutorial-with-the-keras-deep-learning-library-d7f93fa05537)\n",
    "<br>You'll recognize the same steps of: cleaning / pre-processing data, splitting the data into training and testing sets, creating a neural network, training the model, and evaluate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
